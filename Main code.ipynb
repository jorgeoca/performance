{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fae09a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.dummy import DummyRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "pd.set_option('mode.chained_assignment', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3899b8c3",
   "metadata": {},
   "source": [
    "# Participant Descriptors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3154a11f",
   "metadata": {},
   "outputs": [],
   "source": [
    "part_desc = pd.read_excel('Data/participant_descriptors_original.xlsx')\n",
    "part_desc.dropna(axis='index', how='all', inplace=True)\n",
    "part_desc.dropna(axis='columns', how='all', inplace=True)\n",
    "part_desc.reset_index(inplace=True, drop=True)\n",
    "part_desc.iloc[:, : ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "211c70bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "part_desc.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1042a3d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "part_desc = part_desc.drop(['Dataset', 'Study', 'Initials'], axis=1)\n",
    "part_desc.columns = ['sex', 'age(y)', 'bm(kg)', 'height(m)', 'vo2_peak(L.min-1)', 'peak_power(W)', 'get(L.min-1)', 'get(W)']\n",
    "part_desc = part_desc.fillna(0.0)\n",
    "part_desc.iloc[:, : ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c6dbfe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(part_desc['get(L.min-1)'].min())\n",
    "print(part_desc['get(L.min-1)'].max())\n",
    "\n",
    "def format_value(x):\n",
    "    if isinstance(x, (int, float)):\n",
    "        if x < 100:\n",
    "            return float('{:.3f}'.format(x))\n",
    "        else:\n",
    "            return float('{:.3f}'.format(x / 1000))\n",
    "    else:\n",
    "        return x\n",
    "    \n",
    "part_desc['get(L.min-1)'] = part_desc['get(L.min-1)'].apply(format_value)\n",
    "\n",
    "print(part_desc['get(L.min-1)'].min())\n",
    "print(part_desc['get(L.min-1)'].max())\n",
    "\n",
    "#1 is female, 2 is male\n",
    "part_desc['sex'] = part_desc['sex'].replace({'F': 1, 'M': 2}).astype(int)\n",
    "part_desc['age(y)'] = part_desc['age(y)'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ee691d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "part_desc.hist(layout=(4, 4), figsize=(10, 10))\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b6e365f",
   "metadata": {},
   "source": [
    "# Power"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a76e18c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "power = pd.read_excel('Data/power_original.xlsx')\n",
    "power.dropna(axis='index', how='all', inplace=True)\n",
    "power.dropna(axis='columns', how='all', inplace=True)\n",
    "power = power.iloc[:-1 , 1:]\n",
    "power.reset_index(inplace=True, drop=True)\n",
    "power.iloc[:, : ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c92ece1",
   "metadata": {},
   "outputs": [],
   "source": [
    "power.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a09cb17",
   "metadata": {},
   "outputs": [],
   "source": [
    "power_trans = power.T\n",
    "power_trans.reset_index(inplace=True, drop=True)\n",
    "power_trans.iloc[:, : ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09fada1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('There are', power_trans.isnull().sum().sum(), 'missing values')\n",
    "\n",
    "# Find empty cells\n",
    "power_empty_rows = 0\n",
    "for index, row in power_trans.iterrows():\n",
    "    if row.isnull().all():\n",
    "        power_empty_rows = cadence_empty_rows + 1\n",
    "        continue\n",
    "        \n",
    "    # Fill empty values with the average of the preceding and succeeding non-empty values\n",
    "    for i in range(len(row)-1, -1, -1):\n",
    "        if pd.isnull(row[i]):\n",
    "            j = i - 1\n",
    "            while pd.isnull(row[j]):\n",
    "                j -= 1\n",
    "            row[i] = row[j]\n",
    "    \n",
    "    power_trans.iloc[index] = row\n",
    "    \n",
    "print('There are', (power_trans.isnull().sum().sum()) - (power_empty_rows * 180), 'missing values')\n",
    "print('There are', power_empty_rows, 'empty rows')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce6d34b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "power_flatten_data = power_trans.to_numpy().flatten().astype(float)\n",
    "plt.hist(power_flatten_data, bins=40)\n",
    "plt.xlabel('Power')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63d59b4b",
   "metadata": {},
   "source": [
    "# Cadence "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f762be31",
   "metadata": {},
   "outputs": [],
   "source": [
    "cadence = pd.read_excel('Data/cadence_original.xlsx')\n",
    "cadence.dropna(axis='index', how='all', inplace=True)\n",
    "cadence.dropna(axis='columns', how='all', inplace=True)\n",
    "cadence = cadence.iloc[:-1 , 1:]\n",
    "cadence.reset_index(inplace=True, drop=True)\n",
    "cadence.iloc[:, : ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "458011f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "cadence.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17ba93cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "cadence_trans = cadence.T\n",
    "cadence_trans.reset_index(inplace=True, drop=True)\n",
    "cadence_trans.iloc[:, : ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7c211e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('There are', cadence_trans.isnull().sum().sum(), 'missing values')\n",
    "\n",
    "# Find empty cells\n",
    "cadence_empty_rows = 0\n",
    "for index, row in cadence_trans.iterrows():\n",
    "    if row.isnull().all():\n",
    "        cadence_empty_rows = cadence_empty_rows + 1\n",
    "        continue\n",
    "        \n",
    "    # Fill empty values with the average of the preceding and succeeding non-empty values\n",
    "    for i in range(len(row)-1, -1, -1):\n",
    "        if pd.isnull(row[i]):\n",
    "            j = i - 1\n",
    "            while pd.isnull(row[j]):\n",
    "                j -= 1\n",
    "            row[i] = row[j]\n",
    "    \n",
    "    cadence_trans.iloc[index] = row\n",
    "    \n",
    "print('There are', (cadence_trans.isnull().sum().sum()) - (cadence_empty_rows * 180), 'missing values')\n",
    "print('There are', cadence_empty_rows, 'empty rows')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb26a60f",
   "metadata": {},
   "outputs": [],
   "source": [
    "cadence_flatten_data = cadence_trans.to_numpy().flatten().astype(float)\n",
    "plt.xlabel('Cadence')\n",
    "plt.hist(cadence_flatten_data, bins=40)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cf3dd51",
   "metadata": {},
   "source": [
    "# VO2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2081805",
   "metadata": {},
   "outputs": [],
   "source": [
    "vo2 = pd.read_excel('Data/vo2_original.xlsx')\n",
    "cadence.dropna(axis='index', how='all', inplace=True)\n",
    "cadence.dropna(axis='columns', how='all', inplace=True)\n",
    "vo2 = vo2.iloc[: , 1:]\n",
    "vo2.reset_index(inplace=True, drop=True)\n",
    "vo2.iloc[:, : ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1210ce4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "vo2.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d605e52",
   "metadata": {},
   "outputs": [],
   "source": [
    "vo2_trans = vo2.T.astype(float)\n",
    "vo2_trans.reset_index(inplace=True, drop=True)\n",
    "vo2_trans.iloc[:, : ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a037d08e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(vo2_trans.min().min())\n",
    "print(vo2_trans.max().max())\n",
    "\n",
    "def format_value(x):\n",
    "    if isinstance(x, (int, float)):\n",
    "        if x < 100 and x > 0:\n",
    "            return float('{:.3f}'.format(x))\n",
    "        elif x > 100:\n",
    "            return float('{:.3f}'.format(x / 1000))\n",
    "        else:\n",
    "            return x\n",
    "    else:\n",
    "        return x\n",
    "    \n",
    "vo2_trans = vo2_trans.applymap(format_value)\n",
    "\n",
    "print(vo2_trans.min().min())\n",
    "print(vo2_trans.max().max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b95fd3c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "vo2_flatten_data = vo2_trans.to_numpy().flatten().astype(float)\n",
    "plt.hist(vo2_flatten_data, bins=40)\n",
    "plt.xlabel('VO2')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebe6ba87",
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = range(9, 180, 10) \n",
    "\n",
    "# Contains only rows 10, 20, 30, ...\n",
    "vo2_trans_2 = vo2_trans.iloc[:, indices]\n",
    "\n",
    "x_values = vo2_trans_2[130:131].values\n",
    "y_values = vo2_trans_2.columns\n",
    "\n",
    "plt.scatter(y_values, x_values)\n",
    "plt.xlabel('Time (s)')\n",
    "plt.ylabel('VO2')\n",
    "plt.ylim(0, 5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27270d94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find empty cells\n",
    "vo2_empty_rows = 0\n",
    "for index, row in vo2_trans_2.iterrows():\n",
    "    if row.isnull().all():\n",
    "        vo2_empty_rows += 1\n",
    "        continue\n",
    "        \n",
    "    # Fill empty values with the average of the preceding and succeeding non-empty values\n",
    "    for i in range(179, 8, -10):\n",
    "        if pd.isnull(row[i]):\n",
    "            j = i\n",
    "            while j > 9 and pd.isnull(row[j]):\n",
    "                j -= 10\n",
    "                if pd.notnull(row[j]):\n",
    "                    row[i] = row[j]\n",
    "            if j <= 9:\n",
    "                j = i\n",
    "                j += 10\n",
    "                if pd.notnull(row[j]):\n",
    "                    row[i] = row[j]\n",
    "                    \n",
    "    vo2_trans_2_copy = vo2_trans_2.copy()\n",
    "    vo2_trans_2_copy.loc[index] = row\n",
    "    \n",
    "print('There are', (vo2_trans_2_copy.isnull().sum().sum()) - (vo2_empty_rows * 18), 'missing values')\n",
    "print('There are', vo2_empty_rows, 'empty rows')\n",
    "\n",
    "#Fills empty rows with 0s\n",
    "vo2_trans_2_copy = vo2_trans_2_copy.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e576a4bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interpolate vo2\n",
    "new_columns = list(range(0, 180))\n",
    "vo2_interpolated = vo2_trans_2_copy.reindex(columns=new_columns).interpolate(method='linear', axis=1)\n",
    "vo2_interpolated.loc[:, :1] = vo2_interpolated.loc[:, :1].fillna(0)\n",
    "vo2_interpolated.interpolate(method='linear', axis=1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d25b501d",
   "metadata": {},
   "outputs": [],
   "source": [
    "vo2_flatten_data = vo2_interpolated.to_numpy().flatten().astype(float)\n",
    "\n",
    "# plot a histogram of all the data\n",
    "plt.hist(vo2_flatten_data, bins=40)\n",
    "plt.xlabel('VO2')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69f3f48e",
   "metadata": {},
   "source": [
    "# CP & W'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0bfc269",
   "metadata": {},
   "outputs": [],
   "source": [
    "power_cp = power_trans.iloc[:, -30:]\n",
    "cp = power_cp.mean(axis=1)\n",
    "w = (power_trans.sum(axis=1) - cp*180)/1000\n",
    "\n",
    "# Add CP and W' to part_desc df\n",
    "part_desc_complete = part_desc.copy()\n",
    "part_desc_complete['CP'] = cp\n",
    "part_desc_complete['w'] = w\n",
    "\n",
    "# Add CP and W' to power df\n",
    "power_trans_complete = power_trans.copy()\n",
    "power_trans_complete.columns = power_trans_complete.columns.astype(str)\n",
    "power_trans_complete.columns = 'power_' + power_trans_complete.columns\n",
    "power_trans_complete['CP'] = cp\n",
    "power_trans_complete['w'] = w\n",
    "\n",
    "# Add CP and W' to cadence df\n",
    "cadence_trans_complete = cadence_trans.copy()\n",
    "cadence_trans_complete.columns = cadence_trans_complete.columns.astype(str)\n",
    "cadence_trans_complete.columns = 'cadence_' + cadence_trans_complete.columns\n",
    "cadence_trans_complete['CP'] = cp\n",
    "cadence_trans_complete['w'] = w\n",
    "\n",
    "# Add CP and W' to vo2 df\n",
    "vo2_trans_complete = vo2_interpolated.copy()\n",
    "vo2_trans_complete.columns = vo2_trans_complete.columns.astype(str)\n",
    "vo2_trans_complete.columns = 'vo2_' + vo2_trans_complete.columns\n",
    "vo2_trans_complete['CP'] = cp\n",
    "vo2_trans_complete['w'] = w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "501f5828",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "median_values = power_trans.median(axis=1)\n",
    "mean_values = power_trans.mean(axis=1)\n",
    "\n",
    "ordered_cp = cp.sort_values(ascending=False)\n",
    "matched_median_values = median_values.loc[ordered_cp.index]\n",
    "matched_mean_values = mean_values.loc[ordered_cp.index]\n",
    "\n",
    "plt.plot(ordered_cp.values, label='CP', color='orange')\n",
    "plt.plot(matched_median_values.values, label='Median Power', color='red')\n",
    "plt.plot(matched_mean_values.values, label='Mean Power', color='blue')\n",
    "plt.xlabel('Participant ID') \n",
    "plt.ylabel('Power')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a40d2b57",
   "metadata": {},
   "source": [
    "## Correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc52233e",
   "metadata": {},
   "outputs": [],
   "source": [
    "corr = part_desc_complete.corr()\n",
    "corr.style.background_gradient(cmap='coolwarm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdf4ff28",
   "metadata": {},
   "outputs": [],
   "source": [
    "corr1 = power_trans_complete.corr()\n",
    "corr1.style.background_gradient(cmap='coolwarm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05edb0f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "corr2 = cadence_trans_complete.corr()\n",
    "corr2.style.background_gradient(cmap='coolwarm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbfdfbc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "corr3 = vo2_trans_complete.corr()\n",
    "corr3.style.background_gradient(cmap='coolwarm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49ace8e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Column names\n",
    "part_desc_complete_altered = part_desc_complete[~(part_desc_complete == 0).any(axis=1)]\n",
    "column_names = part_desc_complete_altered.columns.tolist()\n",
    "column_names.remove('CP')\n",
    "\n",
    "# Plot each column against 'CP' \n",
    "for column in column_names:\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.scatter(part_desc_complete_altered['CP'], part_desc_complete_altered[column], label=column)\n",
    "    plt.xlabel('CP')\n",
    "    plt.ylabel(column)\n",
    "    plt.title(f'Plot of {column} vs CP')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f514ca98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Column names\n",
    "column_names = part_desc_complete_altered.columns.tolist()\n",
    "column_names.remove('w')\n",
    "\n",
    "# Plot each column against 'w' \n",
    "for column in column_names:\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.scatter(part_desc_complete_altered['w'], part_desc_complete_altered[column], label=column)\n",
    "    plt.xlabel(\"W'\")\n",
    "    plt.ylabel(column)\n",
    "    plt.title(f\"Plot of {column} vs W'\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1433b358",
   "metadata": {},
   "source": [
    "# Feature Engineer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffc11ac0",
   "metadata": {},
   "source": [
    "### Power cumulative sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58e94fdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "power_cumulative_df = power_trans_complete.copy()\n",
    "power_cumulative_df.columns = 'cumulative_' + power_cumulative_df.columns\n",
    "\n",
    "# Save CP and W values\n",
    "CP_and_W_p = power_cumulative_df.iloc[:,-2:]\n",
    "\n",
    "# Calculate gradients for each participant (row)\n",
    "power_cumulative_df = power_cumulative_df.iloc[:,:-2].cumsum(axis=1)\n",
    "power_cumulative_df = power_cumulative_df.fillna(0)\n",
    "power_cumulative_df['CP'] = cp\n",
    "power_cumulative_df['w'] = w"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acba284b",
   "metadata": {},
   "source": [
    "### Power derivative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4a19544",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate derivatives (gradients) for each participant\n",
    "power_derivatives_df = power_cumulative_df.iloc[:, :-2].diff(axis=1)\n",
    "power_derivatives_df.columns = 'derivative_of_' + power_derivatives_df.columns\n",
    "power_derivatives_df['CP'] = cp\n",
    "power_derivatives_df['w'] = w"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "858a84e6",
   "metadata": {},
   "source": [
    "### Cadence cumulative sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8d52d62",
   "metadata": {},
   "outputs": [],
   "source": [
    "cadence_cumulative_df = cadence_trans_complete.copy()\n",
    "cadence_cumulative_df.columns = 'cumulative_' + cadence_cumulative_df.columns\n",
    "\n",
    "# Save CP and W values\n",
    "CP_and_W_p = cadence_cumulative_df.iloc[:,-2:]\n",
    "\n",
    "# Calculate gradients for each participant (row)\n",
    "cadence_cumulative_df = cadence_cumulative_df.iloc[:,:-2].cumsum(axis=1)\n",
    "cadence_cumulative_df = cadence_cumulative_df.fillna(0)\n",
    "cadence_cumulative_df['CP'] = cp\n",
    "cadence_cumulative_df['w'] = w"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7670a86f",
   "metadata": {},
   "source": [
    "### Cadence derivative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d242a40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate derivatives (gradients) for each participant\n",
    "cadence_derivatives_df = cadence_cumulative_df.iloc[:, :-2].diff(axis=1)\n",
    "cadence_derivatives_df.columns = 'derivative_of_' + cadence_derivatives_df.columns\n",
    "cadence_derivatives_df['CP'] = cp\n",
    "cadence_derivatives_df['w'] = w"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39d09059",
   "metadata": {},
   "source": [
    "### VO2 cumulative sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ee690ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "vo2_cumulative_df = vo2_trans_complete.copy()\n",
    "vo2_cumulative_df.columns = 'cumulative_' + vo2_cumulative_df.columns\n",
    "\n",
    "# Save CP and W values\n",
    "CP_and_W_p = vo2_cumulative_df.iloc[:,-2:]\n",
    "\n",
    "# Calculate gradients for each participant (row)\n",
    "vo2_cumulative_df = vo2_cumulative_df.iloc[:,:-2].cumsum(axis=1)\n",
    "vo2_cumulative_df = vo2_cumulative_df.fillna(0)\n",
    "vo2_cumulative_df['CP'] = cp\n",
    "vo2_cumulative_df['w'] = w"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff9cfedb",
   "metadata": {},
   "source": [
    "### VO2 derivative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b943454",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate derivatives (gradients) for each participant\n",
    "vo2_derivatives_df = vo2_cumulative_df.iloc[:, :-2].diff(axis=1)\n",
    "vo2_derivatives_df.columns = 'derivative_of_' + vo2_derivatives_df.columns\n",
    "\n",
    "vo2_derivatives_df['CP'] = cp\n",
    "vo2_derivatives_df['w'] = w"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98797d6f",
   "metadata": {},
   "source": [
    "# Group rows by participant "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0caf2614",
   "metadata": {},
   "outputs": [],
   "source": [
    "part_desc_grouped = part_desc_complete.copy()\n",
    "part_desc_grouped = part_desc_grouped[part_desc_grouped['vo2_peak(L.min-1)'] != 0]\n",
    "part_desc_grouped['Individual'] = pd.factorize(part_desc_grouped['sex'].astype(str) + part_desc_grouped['age(y)'].astype(str) + part_desc_grouped['bm(kg)'].astype(str) + part_desc_grouped['height(m)'].astype(str) + part_desc_grouped['vo2_peak(L.min-1)'].astype(str) + part_desc_grouped['peak_power(W)'].astype(str) + part_desc_grouped['get(L.min-1)'].astype(str))[0] + 1\n",
    "part_desc_grouped = part_desc_grouped[['Individual', 'sex', 'age(y)', 'bm(kg)', 'height(m)', 'vo2_peak(L.min-1)', 'peak_power(W)', 'get(L.min-1)', 'get(W)', 'CP', 'w']]\n",
    "part_desc_ready = part_desc_grouped[~(part_desc_grouped == 0).any(axis=1)]\n",
    "individuals = part_desc_grouped['Individual']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "931c3194",
   "metadata": {},
   "outputs": [],
   "source": [
    "power_trans_grouped = power_trans_complete.copy()\n",
    "power_trans_grouped = power_trans_grouped.loc[~(power_trans_grouped == 0).all(axis=1)]\n",
    "power_trans_grouped.insert(0, 'Individual', individuals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48e3b134",
   "metadata": {},
   "outputs": [],
   "source": [
    "cadence_trans_grouped = cadence_trans_complete.copy()\n",
    "cadence_trans_grouped = cadence_trans_grouped.loc[~(cadence_trans_grouped == 0).all(axis=1)]\n",
    "cadence_trans_grouped.insert(0, 'Individual', individuals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66c5fc54",
   "metadata": {},
   "outputs": [],
   "source": [
    "vo2_trans_grouped = vo2_trans_complete.copy()\n",
    "vo2_trans_grouped = vo2_trans_grouped.loc[~(vo2_trans_grouped.iloc[:,1:-2] == 0).all(axis=1)]\n",
    "vo2_trans_grouped.insert(0, 'Individual', individuals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89f99af1",
   "metadata": {},
   "outputs": [],
   "source": [
    "power_cumulative_grouped = power_cumulative_df.copy()\n",
    "power_cumulative_grouped = power_cumulative_grouped.loc[~(power_cumulative_grouped == 0).all(axis=1)]\n",
    "power_cumulative_grouped.insert(0, 'Individual', individuals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d97916e",
   "metadata": {},
   "outputs": [],
   "source": [
    "power_derivative_grouped = power_derivatives_df.copy()\n",
    "power_derivative_grouped = power_derivative_grouped.loc[~(power_derivative_grouped == 0).all(axis=1)]\n",
    "power_derivative_grouped.insert(0, 'Individual', individuals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff65485c",
   "metadata": {},
   "outputs": [],
   "source": [
    "cadence_derivative_grouped = cadence_derivatives_df.copy()\n",
    "cadence_derivative_grouped = cadence_derivative_grouped.loc[~(cadence_derivative_grouped == 0).all(axis=1)]\n",
    "cadence_derivative_grouped.insert(0, 'Individual', individuals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a5ab51c",
   "metadata": {},
   "outputs": [],
   "source": [
    "vo2_derivative_grouped = vo2_derivatives_df.copy()\n",
    "vo2_derivative_grouped = vo2_derivative_grouped.loc[~(vo2_derivative_grouped == 0).all(axis=1)]\n",
    "vo2_derivative_grouped.insert(0, 'Individual', individuals)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c45b532",
   "metadata": {},
   "source": [
    "## Participant Descriptors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5affa132",
   "metadata": {},
   "source": [
    "part_desc_ready"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acb62227",
   "metadata": {},
   "outputs": [],
   "source": [
    "part_desc_ready"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9cef149",
   "metadata": {},
   "source": [
    "## Number of seconds needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8be17b91",
   "metadata": {},
   "outputs": [],
   "source": [
    "secs = -122"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eee1caa1",
   "metadata": {},
   "source": [
    "## Power"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4d6109f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save CP and W values\n",
    "CP_and_W_p = power_trans_grouped.iloc[:,-2:]\n",
    "\n",
    "# Select how many seconds are needed for power\n",
    "power = power_trans_grouped.iloc[:,:secs]\n",
    "\n",
    "# Add CP and W back to df\n",
    "power = power.join(CP_and_W_p)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92b3c5d6",
   "metadata": {},
   "source": [
    "## Power Peaks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb99f29c",
   "metadata": {},
   "outputs": [],
   "source": [
    "power_peak = power.iloc[:,:-2].max(axis=1)\n",
    "power_peak = pd.DataFrame(power_peak, columns=['Power_Peak'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1acadadc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the columns\n",
    "column1 = cp\n",
    "column2 = power_peak.iloc[:, 0]\n",
    "\n",
    "# Create a scatter plot\n",
    "plt.scatter(column1, column2)\n",
    "plt.xlabel('Column from df1')\n",
    "plt.ylabel('Column from df2')\n",
    "plt.title('Scatter Plot: df1 vs df2')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce021612",
   "metadata": {},
   "source": [
    "## Cadence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f132b27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save CP and W values\n",
    "CP_and_W_c = cadence_trans_grouped.iloc[:,-2:]\n",
    "\n",
    "# Select how many seconds are needed for cadence\n",
    "cadence = cadence_trans_grouped.iloc[:,:secs]\n",
    "\n",
    "# Add CP and W back to df\n",
    "cadence = cadence.join(CP_and_W_c)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a1a623d",
   "metadata": {},
   "source": [
    "## Cadence Peaks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e24cc5ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "cadence_peak = cadence.iloc[:,:-2].max(axis=1)\n",
    "cadence_peak = pd.DataFrame(cadence_peak, columns=['Cadence_Peak'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7908119d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the columns\n",
    "column1 = cadence.iloc[:,-2:-1]\n",
    "column2 = cadence_peak.iloc[:, 0]\n",
    "\n",
    "# Create a scatter plot\n",
    "plt.scatter(column1, column2)\n",
    "plt.xlabel('CP')\n",
    "plt.ylabel('Cadence Peak')\n",
    "plt.title('Scatter Plot: df1 vs df2')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5122dbd3",
   "metadata": {},
   "source": [
    "## VO2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b24a8262",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save CP and W values\n",
    "CP_and_W_v = vo2_trans_grouped.iloc[:,-2:]\n",
    "\n",
    "# Select how many seconds are needed for vo2\n",
    "vo2 = vo2_trans_grouped.iloc[:,:secs]\n",
    "\n",
    "# Add CP and W back to df\n",
    "vo2 = vo2.join(CP_and_W_v)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cce477d",
   "metadata": {},
   "source": [
    "## VO2 Peaks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40ef5dbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "vo2_peak = vo2.iloc[:,:-2].max(axis=1)\n",
    "vo2_peak = pd.DataFrame(vo2_peak, columns=['VO2_Peak'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e553cbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the columns\n",
    "column1 = vo2.iloc[:,-2:-1]\n",
    "column2 = vo2_peak.iloc[:, 0]\n",
    "\n",
    "# Create a scatter plot\n",
    "plt.scatter(column1, column2)\n",
    "plt.xlabel('CP')\n",
    "plt.ylabel('VO2 Peak')\n",
    "plt.title('Scatter Plot: CP vs VO2_Peak')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dfa9a2b",
   "metadata": {},
   "source": [
    "## Participant Descriptors and Power"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "215fd497",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join df\n",
    "part_desc_power_joined = part_desc_ready.iloc[:,:-2].join(power_trans_grouped.iloc[:,1:])\n",
    "part_desc_power_joined = part_desc_power_joined.dropna(how='any')\n",
    "\n",
    "# Save CP and W values\n",
    "CP_and_W_pdp = part_desc_power_joined.iloc[:,-2:]\n",
    "\n",
    "# Select how many seconds are needed\n",
    "part_desc_power_joined = part_desc_power_joined.iloc[:,:secs]\n",
    "\n",
    "# Add CP and W back to df\n",
    "part_desc_power_joined = part_desc_power_joined.join(CP_and_W_pdp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce9fb179",
   "metadata": {},
   "source": [
    "## Participant Descriptors, Power, and Cadence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c88a2f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join df and select how many seconds are needed\n",
    "part_desc_power_cadence_joined = part_desc_ready.iloc[:,:-2].join(power_trans_grouped.iloc[:,1:secs]).join(cadence_trans_grouped.iloc[:,1:])\n",
    "part_desc_power_cadence_joined = part_desc_power_cadence_joined.dropna(how='any')\n",
    "\n",
    "# Save CP and W values\n",
    "CP_and_W_pdpc = part_desc_power_cadence_joined.iloc[:,-2:]\n",
    "\n",
    "# Select how many seconds are needed\n",
    "part_desc_power_cadence_joined = part_desc_power_cadence_joined.iloc[:,:secs]\n",
    "\n",
    "# Add CP and W back to df\n",
    "part_desc_power_cadence_joined = part_desc_power_cadence_joined.join(CP_and_W_pdpc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e998f74",
   "metadata": {},
   "source": [
    "## Participant Descriptors, Power, Cadence, and VO2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d1f1f88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join df and select how many seconds are needed\n",
    "part_desc_power_cadence_vo2_joined = part_desc_ready.iloc[:,:-2].join(power_trans_grouped.iloc[:,1:secs]).join(cadence_trans_grouped.iloc[:,1:secs]).join(vo2_trans_grouped.iloc[:,1:])\n",
    "part_desc_power_cadence_vo2_joined = part_desc_power_cadence_vo2_joined.dropna(how='any')\n",
    "\n",
    "# Save CP and W values\n",
    "CP_and_W_pdpcv = part_desc_power_cadence_vo2_joined.iloc[:,-2:]\n",
    "\n",
    "# Select how many seconds are needed\n",
    "part_desc_power_cadence_vo2_joined = part_desc_power_cadence_vo2_joined.iloc[:,:secs]\n",
    "\n",
    "# Add CP and W back to df\n",
    "part_desc_power_cadence_vo2_joined = part_desc_power_cadence_vo2_joined.join(CP_and_W_pdpcv)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6763d771",
   "metadata": {},
   "source": [
    "## Participant Descriptors, Power, Power Cumulative Sum, Cadence, VO2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6628d6f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join df and select how many seconds are needed\n",
    "part_desc_power_variants_cadence_vo2_joined = part_desc_ready.iloc[:,:-2].join(power_trans_grouped.iloc[:,1:secs]).join(power_cumulative_grouped.iloc[:,1:secs]).join(cadence_trans_grouped.iloc[:,1:secs]).join(vo2_trans_grouped.iloc[:,1:])\n",
    "part_desc_power_variants_cadence_vo2_joined = part_desc_power_variants_cadence_vo2_joined.dropna(how='any')\n",
    "\n",
    "# Save CP and W values\n",
    "CP_and_W_pdpcv = part_desc_power_variants_cadence_vo2_joined.iloc[:,-2:]\n",
    "\n",
    "# Select how many seconds are needed\n",
    "part_desc_power_variants_cadence_vo2_joined = part_desc_power_variants_cadence_vo2_joined.iloc[:,:secs]\n",
    "\n",
    "# Add CP and W back to df\n",
    "part_desc_power_variants_cadence_vo2_joined = part_desc_power_variants_cadence_vo2_joined.join(CP_and_W_pdpcv)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52a9caff",
   "metadata": {},
   "source": [
    "## Participant Descriptors, Power, Peaks, Power and Cadence Derivatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29a46495",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join df and select how many seconds are needed\n",
    "part_desc_peaks_derivatives_joined = part_desc_ready.iloc[:,:-2].join(power_trans_grouped.iloc[:,1:secs]).join(power_peak).join(cadence_peak).join(vo2_peak).join(power_derivative_grouped.iloc[:,2:secs]).join(cadence_derivative_grouped.iloc[:,2:])\n",
    "part_desc_peaks_derivatives_joined = part_desc_peaks_derivatives_joined.dropna(how='any')\n",
    "\n",
    "# Save CP and W values\n",
    "CP_and_W_pdpd = part_desc_peaks_derivatives_joined.iloc[:,-2:]\n",
    "\n",
    "# Select how many seconds are needed\n",
    "part_desc_peaks_derivatives_joined = part_desc_peaks_derivatives_joined.iloc[:,:secs]\n",
    "\n",
    "# Add CP and W back to df\n",
    "part_desc_peaks_derivatives_joined = part_desc_peaks_derivatives_joined.join(CP_and_W_pdpd)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fcd6e3d",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ee67ba4",
   "metadata": {},
   "source": [
    "### Train / Test  split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ec83b91",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split(input_df, target):\n",
    "    # Shuffle the DataFrame to randomize the order\n",
    "    shuffling_df = input_df.sample(frac=1, random_state=42)\n",
    "\n",
    "    # Identify individuals and the number of tests they have done\n",
    "    individuals_counts_df = shuffling_df['Individual'].value_counts()\n",
    "\n",
    "    # Create an empty list to store individuals for train and test splits\n",
    "    train_individuals_df = []\n",
    "    test_individuals_df = []\n",
    "\n",
    "    # Iterate through individuals and add them to the splits while keeping individuals together\n",
    "    for individual in individuals_counts_df.index:\n",
    "        if len(train_individuals_df) / len(shuffling_df) < 0.7:\n",
    "            train_individuals_df.extend([individual] * individuals_counts_df[individual])\n",
    "        else:\n",
    "            test_individuals_df.extend([individual] * individuals_counts_df[individual])\n",
    "\n",
    "    # Create the final train and test DataFrames by selecting all rows corresponding to the selected individuals\n",
    "    train_df = shuffling_df[shuffling_df['Individual'].isin(train_individuals_df)]\n",
    "    test_df = shuffling_df[shuffling_df['Individual'].isin(test_individuals_df)]\n",
    "    \n",
    "    # Create X_train, y_train, X_test, y_test\n",
    "    X_train = train_df.iloc[:,1:-2]\n",
    "    y_train = train_df[[target]]\n",
    "    X_test = test_df.iloc[:,1:-2]\n",
    "    y_test = test_df[[target]]\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "    return X_train_scaled, y_train, X_test_scaled, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bde4493",
   "metadata": {},
   "source": [
    "### Baseline Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5c1a77b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def baseline(X_train, y_train, X_test, y_test):\n",
    "    baseline_model = DummyRegressor(strategy='mean')\n",
    "    baseline_model.fit(X_train, y_train)\n",
    "    \n",
    "    # Make predictions on the test data\n",
    "    y_pred_baseline = baseline_model.predict(X_test)\n",
    "    y_test_baseline = y_test.iloc[:, 0]\n",
    "\n",
    "    # Evaluate the model\n",
    "    mse = mean_squared_error(y_test_baseline, y_pred_baseline)\n",
    "    r2 = r2_score(y_test_baseline, y_pred_baseline)\n",
    "    mape = np.mean(np.abs((y_test_baseline - y_pred_baseline) / y_test_baseline)) * 100\n",
    "    \n",
    "    return mse, r2, mape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "559c43fd",
   "metadata": {},
   "source": [
    "### Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "885762f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_regression(X_train, y_train, X_test, y_test):\n",
    "    linear_model = LinearRegression()\n",
    "    \n",
    "    # Perform cross-validation\n",
    "    mse_cv = -cross_val_score(linear_model, X_train, y_train, scoring='neg_mean_squared_error', cv=5)\n",
    "    r2_cv = cross_val_score(linear_model, X_train, y_train, scoring='r2', cv=5)\n",
    "    mape_cv = cross_val_score(linear_model, X_train, y_train, scoring='neg_mean_absolute_percentage_error', cv=5)\n",
    "    \n",
    "    # Fit the model\n",
    "    linear_model.fit(X_train, y_train)\n",
    "    \n",
    "    y_pred_linear_test = linear_model.predict(X_test)\n",
    "    y_test_linear_test = y_test.values.ravel()\n",
    "    \n",
    "    mse_test = mean_squared_error(y_test_linear_test, y_pred_linear_test)\n",
    "    r2_test = r2_score(y_test_linear_test, y_pred_linear_test)\n",
    "    mape_test = np.mean(np.abs((y_test_linear_test - y_pred_linear_test) / y_test_linear_test)) * 100\n",
    "    \n",
    "    return mse_test, r2_test, mape_test, np.mean(mse_cv), np.mean(r2_cv), -np.mean(mape_cv)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85223ca5",
   "metadata": {},
   "source": [
    "### Decision Tree Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81a57493",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decision_tree_regression(X_train, y_train, X_test, y_test):\n",
    "    decision_tree_model = DecisionTreeRegressor(random_state=42)\n",
    "    \n",
    "    # Perform cross-validation\n",
    "    mse_cv = -cross_val_score(decision_tree_model, X_train, y_train, scoring='neg_mean_squared_error', cv=5)\n",
    "    r2_cv = cross_val_score(decision_tree_model, X_train, y_train, scoring='r2', cv=5)\n",
    "    mape_cv = cross_val_score(decision_tree_model, X_train, y_train, scoring='neg_mean_absolute_percentage_error', cv=5)\n",
    "\n",
    "    # Fit the model\n",
    "    decision_tree_model.fit(X_train, y_train)\n",
    "    \n",
    "    y_pred_decision_tree_test = decision_tree_model.predict(X_test)\n",
    "    y_test_decision_tree_test = y_test.values.ravel()\n",
    "\n",
    "    mse_test = mean_squared_error(y_test_decision_tree_test, y_pred_decision_tree_test)\n",
    "    r2_test = r2_score(y_test_decision_tree_test, y_pred_decision_tree_test)\n",
    "    mape_test = np.mean(np.abs((y_test_decision_tree_test - y_pred_decision_tree_test) / y_test_decision_tree_test)) * 100\n",
    "    \n",
    "    return mse_test, r2_test, mape_test, np.mean(mse_cv), np.mean(r2_cv), -np.mean(mape_cv)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "980e4d95",
   "metadata": {},
   "source": [
    "### Random Forest Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "448676e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_forest_regression(X_train, y_train, X_test, y_test):\n",
    "    random_forest_model = RandomForestRegressor(n_estimators=150, max_depth=10, random_state=42)\n",
    "    \n",
    "    # Perform cross-validation\n",
    "    mse_cv = -cross_val_score(random_forest_model, X_train, y_train.values.ravel(), scoring='neg_mean_squared_error', cv=5)\n",
    "    r2_cv = cross_val_score(random_forest_model, X_train, y_train.values.ravel(), scoring='r2', cv=5)\n",
    "    mape_cv = cross_val_score(random_forest_model, X_train, y_train.values.ravel(), scoring='neg_mean_absolute_percentage_error', cv=5)\n",
    "\n",
    "    # Fit the model\n",
    "    random_forest_model.fit(X_train, y_train.values.ravel())\n",
    "    \n",
    "    y_pred_random_forest_test = random_forest_model.predict(X_test)\n",
    "    y_test_random_forest_test = y_test.values.ravel()\n",
    "\n",
    "    mse_test = mean_squared_error(y_test_random_forest_test, y_pred_random_forest_test)\n",
    "    r2_test = r2_score(y_test_random_forest_test, y_pred_random_forest_test)\n",
    "    mape_test = np.mean(np.abs((y_test_random_forest_test - y_pred_random_forest_test) / y_test_random_forest_test)) * 100\n",
    "    \n",
    "    return mse_test, r2_test, mape_test, np.mean(mse_cv), np.mean(r2_cv), -np.mean(mape_cv)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "427e9895",
   "metadata": {},
   "source": [
    "## Only use for hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35546a89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def decision_tree_regression(X_train, y_train, X_test, y_test):\n",
    "#     # Define hyperparameters to tune\n",
    "#     param_grid = {\n",
    "#         'max_depth': [None, 5, 10, 15],\n",
    "#         'min_samples_leaf': [1, 5, 10]\n",
    "#     }\n",
    "\n",
    "#     # Create Decision Tree model\n",
    "#     decision_tree_model = DecisionTreeRegressor(random_state=42)\n",
    "\n",
    "#     # Perform grid search with cross-validation\n",
    "#     grid_search = GridSearchCV(decision_tree_model, param_grid, cv=5, scoring='neg_mean_squared_error')\n",
    "#     grid_search.fit(X_train, y_train)\n",
    "\n",
    "#     # Get the best hyperparameters\n",
    "#     best_params = grid_search.best_params_\n",
    "#     print('Best Hyperparameters:', best_params)\n",
    "    \n",
    "#     # Use the best model for predictions\n",
    "#     best_decision_tree_model = grid_search.best_estimator_\n",
    "    \n",
    "#     # Perform cross-validation\n",
    "#     mse_cv = -cross_val_score(best_decision_tree_model, X_train, y_train, scoring='neg_mean_squared_error', cv=5)\n",
    "#     r2_cv = cross_val_score(best_decision_tree_model, X_train, y_train, scoring='r2', cv=5)\n",
    "#     mape_cv = cross_val_score(best_decision_tree_model, X_train, y_train, scoring='neg_mean_absolute_percentage_error', cv=5)\n",
    "\n",
    "#     # Use the best model for predictions\n",
    "#     y_pred_decision_tree_test = best_decision_tree_model.predict(X_test)\n",
    "#     y_test_decision_tree_test = y_test.values.ravel()\n",
    "\n",
    "#     mse_test = mean_squared_error(y_test_decision_tree_test, y_pred_decision_tree_test)\n",
    "#     r2_test = r2_score(y_test_decision_tree_test, y_pred_decision_tree_test)\n",
    "#     mape_test = np.mean(np.abs((y_test_decision_tree_test - y_pred_decision_tree_test) / y_test_decision_tree_test)) * 100\n",
    "    \n",
    "#     return mse_test, r2_test, mape_test, np.mean(mse_cv), np.mean(r2_cv), -np.mean(mape_cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ece2392",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def random_forest_regression(X_train, y_train, X_test, y_test):\n",
    "#    # Hyperparameters to tune\n",
    "#     param_grid = {\n",
    "#         'n_estimators': [50, 100, 200],\n",
    "#         'max_depth': [5, 10, 15, None],\n",
    "#         'min_samples_leaf': [1, 5, 10]\n",
    "#     }\n",
    "\n",
    "    \n",
    "#     # Create Random Forest model\n",
    "#     random_forest_model = RandomForestRegressor(random_state=42)\n",
    "\n",
    "#     # Perform grid search with cross-validation\n",
    "#     grid_search = GridSearchCV(random_forest_model, param_grid, cv=5, scoring='neg_mean_squared_error')\n",
    "#     grid_search.fit(X_train, y_train.values.ravel())\n",
    "\n",
    "#     # Get the best hyperparameters\n",
    "#     best_params = grid_search.best_params_\n",
    "#     print('Best Hyperparameters:', best_params)\n",
    "    \n",
    "#     best_random_forest_model = grid_search.best_estimator_\n",
    "    \n",
    "#     # Perform cross-validation\n",
    "#     mse_cv = -cross_val_score(best_random_forest_model, X_train, y_train.values.ravel(), scoring='neg_mean_squared_error', cv=5)\n",
    "#     r2_cv = cross_val_score(best_random_forest_model, X_train, y_train.values.ravel(), scoring='r2', cv=5)\n",
    "#     mape_cv = cross_val_score(best_random_forest_model, X_train, y_train.values.ravel(), scoring='neg_mean_absolute_percentage_error', cv=5)\n",
    "    \n",
    "#     # Use the best model for predictions\n",
    "#     y_pred_random_forest_test = best_random_forest_model.predict(X_test)\n",
    "#     y_test_random_forest_test = y_test.values.ravel()\n",
    "\n",
    "#     mse_test = mean_squared_error(y_test_random_forest_test, y_pred_random_forest_test)\n",
    "#     r2_test = r2_score(y_test_random_forest_test, y_pred_random_forest_test)\n",
    "#     mape_test = np.mean(np.abs((y_test_random_forest_test - y_pred_random_forest_test) / y_test_random_forest_test)) * 100\n",
    "    \n",
    "#     return mse_test, r2_test, mape_test, np.mean(mse_cv), np.mean(r2_cv), -np.mean(mape_cv)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2bcb949",
   "metadata": {},
   "source": [
    "### Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba885c7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Use to print results as the function below runs\n",
    "# def print_results(cv, dataset_name, model_name, target, *args):\n",
    "#     if cv == 'Y':\n",
    "#         mse_test, r2_test, mape_test, mse_cv, r2_cv, mape_cv = args\n",
    "#         print(f'{dataset_name} - {model_name} - {target} - Cross-Validation:')\n",
    "#         print('Mean Squared Error (CV):', mse_cv)\n",
    "#         print('R-squared (CV):', r2_cv)\n",
    "#         print('Mean Absolute Percentage Error (MAPE) (CV):', mape_cv)\n",
    "#         print()\n",
    "#         print(f'{dataset_name} - {model_name} - {target} - TEST:')\n",
    "#         print('Mean Squared Error (TEST):', mse_test)\n",
    "#         print('R-squared (TEST):', r2_test)\n",
    "#         print('Mean Absolute Percentage Error (MAPE) (TEST):', mape_test)\n",
    "#         print()\n",
    "#         print('---------------------------------------------------------------')\n",
    "#         print()\n",
    "#     else:\n",
    "#         mse, r2, mape = args\n",
    "#         print(f'{dataset_name} - {model_name} - {target}:')\n",
    "#         print('Mean Squared Error:', mse)\n",
    "#         print('R-squared:', r2)\n",
    "#         print('Mean Absolute Percentage Error (MAPE):', mape)\n",
    "#         print()\n",
    "#         print('---------------------------------------------------------------')\n",
    "#         print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c79ed0e",
   "metadata": {},
   "source": [
    "# Run models and view results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfea9488",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets_names = ['Part_desc', 'Power', 'Cadence', 'VO2', 'Part_desc & Power', 'Part_desc & Power & Cadence', 'Part_desc & Power & Cadence & VO2', 'Part_desc & Power & Variants & Cadence & VO2', 'Part_desc & Peaks & Derivatives']\n",
    "models_names = ['Baseline', 'Linear', 'Decision Tree', 'Random Forest']\n",
    "\n",
    "datasets = [part_desc_ready, power, cadence, vo2, part_desc_power_joined, part_desc_power_cadence_joined, part_desc_power_cadence_vo2_joined, part_desc_power_variants_cadence_vo2_joined, part_desc_peaks_derivatives_joined]\n",
    "models = [baseline, linear_regression, decision_tree_regression, random_forest_regression]\n",
    "targets = ['CP', 'w']\n",
    "\n",
    "# Create an empty list to store the results\n",
    "results_list = []\n",
    "\n",
    "def add_result_to_list(cv, dataset_name, model_name, target, *args):\n",
    "    if cv == 'Y':\n",
    "        mse_test, r2_test, mape_test, mse_cv, r2_cv, mape_cv = args\n",
    "        result_row = {'Model': model_name, 'Dataset': dataset_name, 'Target': target, 'MSE_TEST': mse_test, 'R2_TEST': r2_test, 'MAPE_TEST': mape_test, 'MSE_CV': mse_cv, 'R2_CV': r2_cv, 'MAPE_CV': mape_cv}\n",
    "    else:\n",
    "        mse, r2, mape = args\n",
    "        result_row = {'Model': model_name, 'Dataset': dataset_name, 'Target': target, 'MSE_TEST': mse, 'R2_TEST': r2, 'MAPE_TEST': mape, 'MSE_CV': None, 'R2_CV': None, 'MAPE_CV': None}\n",
    "    results_list.append(result_row)\n",
    "\n",
    "# Loop through each combination and add results to the list\n",
    "for target in targets:\n",
    "    if target == 'CP':\n",
    "        target_name = 'CP'\n",
    "    elif target == 'w':\n",
    "        target_name = 'W'\n",
    "    # Baseline model\n",
    "    for dataset, dataset_name in zip(datasets, datasets_names):\n",
    "#         print_results('N', dataset_name, 'Baseline', target_name, *baseline(*train_test_split(dataset, target)))\n",
    "        add_result_to_list('N', dataset_name, 'Baseline', target_name, *baseline(*train_test_split(dataset, target)))\n",
    "\n",
    "    # All other models\n",
    "    for model, model_name in zip(models[1:], models_names[1:]):\n",
    "        for dataset, dataset_name in zip(datasets, datasets_names):\n",
    "#             print_results('Y', dataset_name, model_name, target_name, *model(*train_test_split(dataset, target)))\n",
    "            add_result_to_list('Y', dataset_name, model_name, target_name, *model(*train_test_split(dataset, target)))\n",
    "\n",
    "# Create a DataFrame from the results list & export to excel\n",
    "results_df = pd.DataFrame(results_list)\n",
    "results_df.to_excel('results.xlsx', index=False)\n",
    "print('Results exported to \"results.xlsx\"')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
